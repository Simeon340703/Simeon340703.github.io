<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Simegnew Yihunie Alaba</title>
  
  <meta name="author" content="Simegnew Yihunie Alaba">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Simegnew Yihunie Alaba</name> 
              </p>
              <p>I am a Ph.D. candidate in the Electrical and Computer Engineering department at Mississippi State University. My research includes computer vision, deep learning, machine learning, and autonomous driving. Follow me for updates.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:simonyhunie@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=p14MNfYAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Simeon340703/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/simegnew-y-alaba">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/sim_dev.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sim_dev.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, deep learning, and autonmous driving. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/SmrNet.png" alt="safs_small" width="220" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.icecet.com/home/">
                <papertitle>SmRNet: Scalable Multiresolution Feature Extraction Network</papertitle>
              </a>
              <br>
              <strong>Simegnew Alaba</strong>, <a href="https://scholar.google.com/citations?user=p14MNfYAAAAJ&hl=en&oi=ao">John E. Ball</a>
              <br>
              <em>ICECET</em>, 2023
              
              <p>This work addresses the challenge of designing a lightweight convolutional neural network (CNN) with high accuracy. To overcome information loss due to pooling operation, the proposed approach leverages the discrete wavelet transform (DWT) and  inverse wavelet transform (IWT) as downsampling and upsampling operators instead of pooling and stride operations. The lost details during downsampling can be effectively restored by exploiting the lossless property of wavelets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/multisensor-fusion.png" alt="safs_small" width="220" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12540/1254005/Multi-sensor-fusion-3D-object-detection-for-autonomous-driving/10.1117/12.2663424.short?SSO=1">
                <papertitle>Multi-sensor fusion 3D object detection for autonomous driving</papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball</a>
              <br>
              <em>Autonomous Systems: Sensors, Processing and Security for Ground, Air, Sea, and Space Vehicles and Infrastructure</em>, 2023
              <br>
              
              <p>  A multi-modal fusion 3D object detection model is proposed for autonomous driving, aiming to leverage the strengths of both LiDAR and camera sensors.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/semi-supervised.png" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430P/Semi-supervised-learning-for-fish-species-recognition/10.1117/12.2663422.short">
                <papertitle>Semi-supervised learning for fish species recognition</papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, Chiranjibi Shah, M. M. Nabi, John E. Ball, Robert Moorhead, Deok Han, Jack Prior, Matthew D. Campbell, Farron Wallace
              <br>
              <em>Ocean Sensing and Monitoring XV</em>, 2023
              <br>
              
              <p> Fish species recognition and detection are essential for fishery industries. Accurate and robust species classification and detection play a vital role in monitoring fish activities and identifying the distribution of a specific species, which is vital to know the endangered species. It is also essential for controlling production and overall ecosystem control and management.In this work, we build a semi-supervised deep-learning network to recognize fish species.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/MI-AFR.png" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430N/MI-AFR--multiple-instance-active-learning-based-approach-for/10.1117/12.2663404.short">
                <papertitle>MI-AFR: multiple instance active learning-based approach for fish species recognition in underwater environments</papertitle>
              </a>
              <br>
              Chiranjibi Shah, <strong>Simegnew Yihunie Alaba</strong>, MM Nabi, Ryan Caillouet, Jack Prior, Matthew Campbell, Farron Wallace, John E Ball, Robert Moorhead
              <br>
              <em>Ocean Sensing and Monitoring XV</em>, 2023
              <br>
              
              <p> Video surveys are commonly used to monitor the abundance and distribution of managed species to support management. However, considerable effort, time, and cost are required for human review and automated fish species recognition provides an effective solution to remove the bottleneck of post-processing. Implementing fish species detection techniques for underwater imagery is a challenging task. In this work, we present the Multiple Instance Active-learning for Fish-species Recognition (MI-AFR), which is formulated as an object detection-based approach to perform localization and classification of fish species.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/yolov5_enhanced.png" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430O/An-enhanced-YOLOv5-model-for-fish-species-recognition-from-underwater/10.1117/12.2663408.short">
                <papertitle>An enhanced YOLOv5 model for fish species recognition from underwater environments</papertitle>
              </a>
              <br>
               Chiranjibi Shah, <strong>Simegnew Yihunie Alaba</strong>, MM Nabi, Jack Prior, Matthew Campbell, Farron Wallace, John E Ball, Robert Moorhead
              <br>
              <em>Ocean Sensing and Monitoring XV</em>, 2023
              <br>
              
              <p> Species recognition is an important aspect of video based surveys, which support stock assessments, inspecting the ecosystem, handling production management, and protecting endangered species. It is a challenging task to implement fish species detection algorithms in underwater environments. In this work, we introduce the YOLOv5 model for the recognition of fish species that can be implemented as an object detection model for analyzing multiple fishes in a single image. Moreover, we have modified the depth scale of different layers in the backbone of the YOLOv5 model to obtain improved results on fish species recognition. In addition, we have implemented a transformer block in the backbone network and introduced a class balance loss function to obtain enhanced performance. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/graphical_abstract.jpg" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10017184">
                <papertitle>Deep Learning-Based Image 3-D Object Detection for Autonomous Driving: Review
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em>IEEE Sensors </em>, 2023
              <br>
              
              <p> An accurate and robust perception system is key to understanding the driving environment of autonomous driving and robots. Autonomous driving needs 3-D information about objects, including the object‚Äôs location and pose, to understand the driving environment clearly. A camera sensor is widely used in autonomous driving because of its richness in color and texture, and low price. The major problem with the camera is the lack of 3-D information, which is necessary to understand the 3-D driving environment. In addition, the object‚Äôs scale change and occlusion make 3-D object detection more challenging. Many deep learning-based methods, such as depth estimation, have been developed to solve the lack of 3-D information. This survey presents the image 3-D object detection 3-D bounding box encoding techniques and evaluation metrics. The image-based methods are categorized based on the technique used to estimate an image‚Äôs depth information, and insights are added to each method. Then, state-of-the-art (SOTA) monocular and stereo camera-based methods are summarized. We also compare the performance of the selected 3-D object detection models and present challenges and future directions in 3-D object detection.
              </p>
            </td>
          </tr>

 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/sensors.png" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/24/9577">
                <papertitle>A Survey on Deep-Learning-Based LiDAR 3D Object Detection for Autonomous Driving 
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em>MDPI Sensing and Imaging </em>, 2022
              <br>
              
              <p>LiDAR is a commonly used sensor for autonomous driving to make accurate, robust, and fast decision-making when driving. The sensor is used in the perception system, especially object detection, to understand the driving environment. Although 2D object detection has succeeded during the deep-learning era, the lack of depth information limits understanding of the driving environment and object location. Three-dimensional sensors, such as LiDAR, give 3D information about the surrounding environment, which is essential for a 3D perception system. Despite the attention of the computer vision community to 3D object detection due to multiple applications in robotics and autonomous driving, there are challenges, such as scale change, sparsity, uneven distribution of LiDAR data, and occlusions. Different representations of LiDAR data and methods to minimize the effect of the sparsity of LiDAR data have been proposed. This survey presents the LiDAR-based 3D object detection and feature-extraction techniques for LiDAR data. The 3D coordinate systems differ in camera and LiDAR-based datasets and methods. Therefore, the commonly used 3D coordinate systems are summarized. Then, state-of-the-art LiDAR-based 3D object-detection methods are reviewed with a selected comparison among methods
              </p>
            </td>
          </tr>

 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/class-aware.jpg" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/21/8268">
                <papertitle>Class-Aware Fish Species Recognition Using Deep Learning for an Imbalanced Dataset
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em> MDPI Sensing and Imaging </em>, 2022
              <br>
              
              <p>Fish species recognition is crucial to identifying the abundance of fish species in a specific area, controlling production management, and monitoring the ecosystem, especially identifying the endangered species, which makes accurate fish species recognition essential. In this work, the fish species recognition problem is formulated as an object detection model to handle multiple fish in a single image, which is challenging to classify using a simple classification network. The proposed model consists of MobileNetv3-large and VGG16 backbone networks and an SSD detection head. Moreover, a class-aware loss function is proposed to solve the class imbalance problem of our dataset. 
              </p>
            </td>
          </tr>

 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/wcnn3d.png" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/18/7010">
                <papertitle>WCNN3D: Wavelet Convolutional Neural Network-Based 3D Object Detection for Autonomous Driving
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em>MDPI Sensing and Imaging </em>, 2022
              <br>
              
              <p>Three-dimensional object detection is crucial for autonomous driving to understand the driving environment. Since the pooling operation causes information loss in the standard CNN, we designed a wavelet-multiresolution-analysis-based 3D object detection network without a pooling operation. Additionally, instead of using a single filter like the standard convolution, we used the lower-frequency and higher-frequency coefficients as a filter. These filters capture more relevant parts than a single filter, enlarging the receptive field. The model comprises a discrete wavelet transform (DWT) and an inverse wavelet transform (IWT) with skip connections to encourage feature reuse for contrasting and expanding layers. The IWT enriches the feature representation by fully recovering the lost details during the downsampling operation. Element-wise summation was used for the skip connections to decrease the computational burden. We trained the model for the Haar and Daubechies (Db4) wavelets. The two-level wavelet decomposition result shows that we can build a lightweight model without losing significant performance. The experimental results on KITTI‚Äôs BEV and 3D evaluation benchmark show that our model outperforms the PointPillars-based model by up to 14% while reducing the number of trainable parameters. 
              </p>
            </td>
          </tr>
 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/seamapd.png" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1Dp9fDjt0KMR_xtRT9pZDEbVuFZpTE_hJ/view">
                <papertitle>SEAMAPD21: a large-scale reef fish dataset for fine-grained categorization
                </papertitle>
              </a>
              <br>
             Oc√©ane Boulais, <strong>Simegnew Yihunie Alaba</strong> John E Ball, Matthew Campbell, Ahmed Tashfin Iftekhar, Robert Moorehead, James Primrose, Jack Prior, Farron Wallace, Henry Yu, Aotian Zheng
              <br>
              <em>FGVC8: The Eight Workshop on Fine-Grained Visual Categorization CVPR </em>, 2021
              <br>
              
              <p>We present the first large-scale, fine-grained reef fish dataset from the Gulf of Mexico-the Southeast Area Monitoring and Assessment Program Dataset 2021 (SEAMAPD21). Automated image analysis of reef fish species in challenging environments, such as coastal Gulf of Mexico, are crucial for accurate fishery monitoring and stock assessments. As advanced computer vision techniques evolve in precision and capacity, previously difficult tasks such as fish species identification and count due to strong intra-species similarities and inter-species variations, have become feasible. SEAMAPD21 was created to enable fishery scientists and researchers to process raw image-based data at high volumes at a much quicker rate than current manual methods. This open-source dataset contains 90,000 annotations for 130 species of fish from two annual surveys done in 2018 and 2019 in strategic areas of the Gulf of Mexico using baited remote underwater video systems to assess reef fish populations. We perform baseline experiments and document predictive classification results. We document this work at https://github. com/SEFSC/SEAMAPD21.
              </p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Memberships</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a >IEEE Memberships</a>
              <br>
              <a >IEEE-Eta Kappa Nu (HKN) Memberships</a>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
