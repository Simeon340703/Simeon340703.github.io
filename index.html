<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Simegnew Yihunie Alaba</title>
  
  <meta name="author" content="Simegnew Yihunie Alaba">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Simegnew Yihunie Alaba</name> 
              </p>
              <p>I am a Ph.D. candidate in the Electrical and Computer Engineering department at Mississippi State University. My research includes computer vision, deep learning, machine learning, and autonomous driving. Follow me for updates.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:simonyhunie@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=p14MNfYAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Simeon340703/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/simegnew-y-alaba">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/sim_dev.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sim_dev.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, deep learning, and autonomous driving. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/SmrNet.png" alt="safs_small" width="220" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.icecet.com/home/">
                <papertitle>SmRNet: Scalable Multiresolution Feature Extraction Network</papertitle>
              </a>
              <br>
              <strong>Simegnew Alaba</strong>, John E. Ball</a>
              <br>
              <em>ICECET, Cape Town-South Africa,  November 16--17 </em>, 2023 (Accepted)
              
              <p>This work addresses the challenge of designing a lightweight convolutional neural network (CNN) with high accuracy.
		      To overcome information loss due to pooling operation, the proposed approach leverages the discrete wavelet transform (DWT) 
		      and  inverse wavelet transform (IWT) as downsampling and upsampling operators instead of pooling and stride operations. 
		      The lost details during downsampling can be effectively restored by exploiting the lossless property of wavelets.</p>
            </td>
          </tr>

<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/zsd_diagram.png" alt="safs_small" width="220" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://gulfcoast23.oceansconference.org/">
                <papertitle>A Zero Shot Detection Based Approach for Fish Species Recognition in Underwater Environments</papertitle>
              </a>
              <br>
              Chiranjibi Shah, M. M. Nabi, <strong>Simegnew Yihunie Alaba</strong>, Jack Prior, Ryan Caillouet, Matthew D. Campbell, Farron Wallace, John E. Ball, Robert Moorhead
              <br>
              <em>IEEE Ocean Conference & Exposition, Biloxi, MS, USA, September 25--28</em> , 2023 (Accepted)
              
              <p>A zero-shot detection for Fish species Recognition (ZSD-FR) in underwater environments is utilized for object detection. 
		      This approach can localize and recognize objects when the model is not trained on ‚Äúunseen‚Äù classes. Generative models like GAN 
		      can be trained on data with ‚Äúseen‚Äù classes for generating unseen class samples depending upon the semantics (attributes) learned 
		      from data with seen classes. </p>
            </td>
          </tr>

<tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/AL_net.png" alt="safs_small" width="220" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://gulfcoast23.oceansconference.org/">
                <papertitle>Probabilistic Model-based Active Learning with Attention Mechanism for Fish Species Recognition</papertitle>
              </a>
              <br>
             M. M. Nabi, Chiranjibi Shah,  <strong>Simegnew Yihunie Alaba</strong>, Jack Prior, Matthew D. Campbell, Farron Wallace,  Robert Moorhead, John E. Ball
              <br>
              <em>IEEE Ocean Conference & Exposition Biloxi, MS, USA, September 25--28</em>, 2023 (Accepted)
              
              <p> This work proposes a deep-learning fish detection and classification model. The model incorporates an attention mechanism
		      named Convolutional Block Attention Module (CBAM) to improve detection performance. A popular Deep Active Learning approach 
		      with cost-efficient annotation is employed, which selects the most informative samples from the unlabeled set. The proposed method utilizes 
		      probabilistic modeling based on mixture density networks to estimate probability distributions for localization and classification heads. </p>
            </td>
          </tr>

	  <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/fish_survey_conference.jpg" alt="safs_small" width="220" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://gulfcoast23.oceansconference.org/">
                <papertitle>Optimizing and Gauging Model Performance with Metrics to Integrate with Existing Video Surveys‚Äù</papertitle>
              </a>
              <br>
               Jack Prior, <strong>Simegnew Yihunie Alaba</strong>,  Farron Wallace, Matthew D. Campbell,  Chiranjibi Shah,  M. M. Nabi,  Paul F. Mickle,
		    Robert Moorhead, John E. Ball
              <br>
              <em>IEEE Ocean Conference & Exposition Biloxi, MS, USA, September 25--28</em>, 2023 (Accepted)
              
              <p> Baited underwater video sampling is a common method to monitor fish populations, yet the data requirements associated with imagery leads to
		      bottlenecks in productivity. Image analysis that incorporates automated methods through deep-learning models could provide solutions.
		      These models have the potential to improve efficiency, and decrease the cost of producing information on fish populations and habitats. 
		      In order to reduce human intervention, these models must produce precise, accurate results. While methods for gauging model performance 
		      through metrics such as mean-average-precision are helpful during the model training process, evaluating the performance on years of survey 
		      data requires a different approach. An otolith age-reader comparison method has been adapted to compare automated counts to true counts.
		      The metrics produced in this analysis are then compared across a span of the model confidence levels in order to find the optimal settings 
		      per species to filter output and improve processing speed. For most species, increasing annotations for model training results in better 
		      performance, however, issues persist with occlusion, turbidity, schooling species, and cryptic/conspecific appearances. 
		      With focus on Red Snapper (Lutjanus campechanus), this process of evaluation was carried out with multiple years of video data to 
		      test for fidelity based on location, time, and environmental conditions. Identifying common failures and adapting active learning algorithms
		      can lead to targeted training for more efficient models in the future. These quality assessment and quality control methods of evaluation 
		      provide a framework for tracking performance drift and integrating automated methods properly with existing surveys and manual video count
		      protocols. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/multisensor-fusion.png" alt="fast-texture" width="220" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12540/1254005/Multi-sensor-fusion-3D-object-detection-for-autonomous-driving/10.1117/12.2663424.short?SSO=1">
                <papertitle>Multi-sensor fusion 3D object detection for autonomous driving</papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball</a>
              <br>
              <em>Autonomous Systems: Sensors, Processing and Security for Ground, Air, Sea, and Space Vehicles and Infrastructure, Orlando, 
		      FL, USA, April 30 ‚Äì May 4</em>, 2023
              <br>
              
              <p>  A multi-modal fusion 3D object detection model is proposed for autonomous driving, aiming to leverage the strengths of
		      both LiDAR and camera sensors.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/semi-supervised.png" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430P/Semi-supervised-learning-for-fish-species-recognition/10.1117/12.2663422.short">
                <papertitle>Semi-supervised learning for fish species recognition</papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, Chiranjibi Shah, M. M. Nabi, John E. Ball, Robert Moorhead, Deok Han, Jack Prior, Matthew D. Campbell, Farron Wallace
              <br>
              <em>Ocean Sensing and Monitoring XV: Machine Learning/Deep Learning 1, Orlando, FL, USA, April 30 ‚Äì May 4</em>, 2023
              <br>
              
              <p> Fish species recognition and detection are essential for fishery industries. Accurate and robust species classification and 
		      detection plays a vital role in monitoring fish activities and identifying the distribution of a specific species, which is vital 
		      to know the endangered species. It is also essential for controlling production and overall ecosystem control and management. 
		      In this work, we build a semi-supervised deep-learning network to recognize fish species.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/MI-AFR.png" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430N/MI-AFR--multiple-instance-active-learning-based-approach-for/10.1117/12.2663404.short">
                <papertitle>MI-AFR: multiple instance active learning-based approach for fish species recognition in underwater environments</papertitle>
              </a>
              <br>
              Chiranjibi Shah, <strong>Simegnew Yihunie Alaba</strong>, MM Nabi, Ryan Caillouet, Jack Prior, Matthew Campbell, Farron Wallace, John E Ball, Robert Moorhead
              <br>
              <em>Ocean Sensing and Monitoring XV: Machine Learning/Deep Learning 1, Orlando, FL, USA, April 30 ‚Äì May 4</em>, 2023
              <br>
              
              <p> Video surveys are commonly used to monitor the abundance and distribution of managed species to support management. 
		      However, considerable effort, time, and cost are required for human review and automated fish species recognition 
		      provides an effective solution to remove the bottleneck of post-processing. Implementing fish species detection techniques 
		      for underwater imagery is a challenging task. In this work, we present the Multiple Instance Active-learning for Fish-species 
		      Recognition (MI-AFR), which is formulated as an object detection-based approach to perform localization and classification of fish species.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/yolov5_enhanced.png" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12543/125430O/An-enhanced-YOLOv5-model-for-fish-species-recognition-from-underwater/10.1117/12.2663408.short">
                <papertitle>An enhanced YOLOv5 model for fish species recognition from underwater environments</papertitle>
              </a>
              <br>
               Chiranjibi Shah, <strong>Simegnew Yihunie Alaba</strong>, MM Nabi, Jack Prior, Matthew Campbell, Farron Wallace, John E Ball, Robert Moorhead
              <br>
              <em>Ocean Sensing and Monitoring XV: Machine Learning/Deep Learning 1, Orlando, FL, USA, April 30 ‚Äì May 4</em>, 2023
              <br>
              
              <p> Species recognition is an important aspect of video-based surveys, which support stock assessments, inspecting the ecosystem,
		      handling production management, and protecting endangered species. It is a challenging task to implement fish species detection 
		      algorithms in underwater environments. In this work, we introduce the YOLOv5 model for the recognition of fish species that can be 
		      implemented as an object detection model for analyzing multiple fishes in a single image. Moreover, we have modified the depth scale of
		      different layers in the backbone of the YOLOv5 model to obtain improved results on fish species recognition. In addition, we have implemented 
		      a transformer block in the backbone network introduced a class balance loss function to enhance performance. 
              </p>
            </td>
          </tr>
 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/fish_survey.jpg" alt="fast-texture" width="220" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.frontiersin.org/articles/10.3389/fmars.2023.1150651/full">
                <papertitle>Estimating precision and accuracy of automated video post-processing: A step towards implementation of AI/ML for optics-based fish sampling</papertitle>
              </a>
              <br>
               Jack Prior, Matthew Campbell,  Matthew Dawkins, Paul F. Mickle,  Robert  J. Moorhead,  <strong>Simegnew Yihunie Alaba</strong>, Chiranjibi Shah, Joseph R. Salisbury,
		    Kevin R. Rademacher, A. Paul Felts, Farron Wallace 
              <br>
              <em>Frontiers in Marine Science</em>, 2023
              <br>
              
              <p> Increased necessity to monitor vital fish habitat has resulted in proliferation of camera-based observation methods and advancements in camera and
		      processing technology. Automated image analysis through computer vision algorithms has emerged as a tool for fisheries to address big data needs, 
		      reduce human intervention, lower costs, and improve timeliness. Models have been developed in this study with the goal to implement such automated image
		      analysis for commercially important Gulf of Mexico fish species and habitats. Further, this study proposes adapting comparative otolith aging
		      methods and metrics for gauging model performance by comparing automated counts to validation set counts in addition to traditional metrics used
		      to gauge AI/ML model performance (such as mean average precision - mAP). To evaluate model performance we calculated percent of stations matching
		      ground-truthed counts, ratios of false-positive/negative detections, and coefficient of variation (CV) for each species over a range of filtered 
		      outputs using model generated confidence thresholds (CTs) for each detected and classified fish.
              </p>
            </td>
          </tr>

	  
          <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/graphical_abstract.jpg" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10017184">
                <papertitle>Deep Learning-Based Image 3-D Object Detection for Autonomous Driving: Review
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em>IEEE Sensors </em>, 2023
              <br>
              
              <p> An accurate and robust perception system is key to understanding the driving environment of autonomous driving and robots. 
		      Autonomous driving needs 3-D information about objects, including the object‚Äôs location and pose, to understand the driving 
		      environment clearly. A camera sensor is widely used in autonomous driving because of its richness in color and texture and low price.
		      The major problem with the camera is the lack of 3-D information, which is necessary to understand the 3-D driving environment. 
		      In addition, the object‚Äôs scale change and occlusion make 3-D object detection more challenging. Many deep learning-based methods, 
		      such as depth estimation, have been developed to solve the lack of 3-D information. This survey presents the image 3-D object detection
		      3-D bounding box encoding techniques and evaluation metrics. The image-based methods are categorized based on the technique used to estimate
		      an image‚Äôs depth information, and insights are added to each method. Then, state-of-the-art (SOTA) monocular and stereo camera-based methods 
		      are summarized. We also compare the performance of the selected 3-D object detection models and present challenges and future directions
		      in 3-D object detection.
              </p>
            </td>
          </tr>

 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/sensors.png" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/24/9577">
                <papertitle>A Survey on Deep-Learning-Based LiDAR 3D Object Detection for Autonomous Driving 
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em>MDPI Sensing and Imaging </em>, 2022
              <br>
              
              <p>LiDAR is a commonly used sensor for autonomous driving to make accurate, robust, and fast decision-making when driving. 
		      The sensor is used in the perception system, especially object detection, to understand the driving environment. 
		      Although 2D object detection has succeeded during the deep-learning era, the lack of depth information limits the understanding of 
		      the driving environment and object location. Three-dimensional sensors, such as LiDAR, give 3D information about the surrounding environment,
		      which is essential for a 3D perception system. Despite the attention of the computer vision community to 3D object detection due to multiple 
		      applications in robotics and autonomous driving, there are challenges, such as scale change, sparsity, uneven distribution of LiDAR data, and
		      occlusions. Different representations of LiDAR data and methods to minimize the effect of the sparsity of LiDAR data have been proposed.
		      This survey presents the LiDAR-based 3D object detection and feature-extraction techniques for LiDAR data. The 3D coordinate systems 
		      differ in camera and LiDAR-based datasets and methods. Therefore, the commonly used 3D coordinate systems are summarized. Then, state-of-the-art 
		      LiDAR-based 3D object-detection methods are reviewed with a selected comparison among methods
              </p>
            </td>
          </tr>

 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/class-aware.jpg" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/21/8268">
                <papertitle>Class-Aware Fish Species Recognition Using Deep Learning for an Imbalanced Dataset
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em> MDPI Sensing and Imaging </em>, 2022
              <br>
              
              <p>Fish species recognition is crucial to identifying the abundance of fish species in a specific area, controlling production management, 
		      and monitoring the ecosystem, especially identifying endangered species, which makes accurate fish species recognition essential.
		      In this work, the fish species recognition problem is formulated as an object detection model to handle multiple fish in a single image,
		      which is challenging to classify using a simple classification network. The proposed model consists of MobileNetv3-large and VGG16 backbone 
		      networks and an SSD detection head. Moreover, a class-aware loss function is proposed to solve the class imbalance problem of our dataset. 
              </p>
            </td>
          </tr>

 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/wcnn3d.png" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/1424-8220/22/18/7010">
                <papertitle>WCNN3D: Wavelet Convolutional Neural Network-Based 3D Object Detection for Autonomous Driving
                </papertitle>
              </a>
              <br>
              <strong>Simegnew Yihunie Alaba</strong>, John E. Ball
              <br>
              <em>MDPI Sensing and Imaging </em>, 2022
              <br>
              
              <p>Three-dimensional object detection is crucial for autonomous driving to understand the driving environment.
		      Since the pooling operation causes information loss in the standard CNN, we designed a wavelet-multiresolution-analysis-based 3D
		      object detection network without a pooling operation. Additionally, instead of using a single filter like the standard convolution,
		      we used the lower-frequency and higher-frequency coefficients as a filter. These filters capture more relevant parts than a single filter, 
		      enlarging the receptive field. The model comprises a discrete wavelet transform (DWT) and an inverse wavelet transform (IWT) with skip connections
		      to encourage feature reuse for contrasting and expanding layers. The IWT enriches the feature representation by fully recovering the lost details
		      \during the downsampling operation. Element-wise summation was used for the skip connections to decrease the computational burden. We trained the 
		      model for the Haar and Daubechies (Db4) wavelets. The two-level wavelet decomposition result shows that we can build a lightweight model without 
		      losing significant performance. The experimental results on KITTI‚Äôs BEV and 3D evaluation benchmark show that our model outperforms the
		      PointPillars-based model by up to 14% while reducing the number of trainable parameters. 
              </p>
            </td>
          </tr>
 <tr>
            <td style="padding:20px;width:40%;vertical-align:middle">
              <img src="images/seamapd.png" alt="fast-texture" width="220" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1Dp9fDjt0KMR_xtRT9pZDEbVuFZpTE_hJ/view">
                <papertitle>SEAMAPD21: a large-scale reef fish dataset for fine-grained categorization
                </papertitle>
              </a>
              <br>
             Oc√©ane Boulais, <strong>Simegnew Yihunie Alaba</strong> John E Ball, Matthew Campbell, Ahmed Tashfin Iftekhar, Robert Moorehead, James Primrose, Jack Prior, Farron Wallace, Henry Yu, Aotian Zheng
              <br>
              <em>FGVC8: The Eight Workshop on Fine-Grained Visual Categorization CVPR </em>, 2021
              <br>
              
              <p>We present the first large-scale, fine-grained reef fish dataset from the Gulf of Mexico-the Southeast Area Monitoring and Assessment
		      Program Dataset 2021 (SEAMAPD21). Automated image analysis of reef fish species in challenging environments, such as coastal Gulf of Mexico, 
		      are crucial for accurate fishery monitoring and stock assessments. As advanced computer vision techniques evolve in precision and capacity, 
		      previously difficult tasks such as fish species identification and count due to strong intra-species similarities and inter-species variations,
		      have become feasible. SEAMAPD21 was created to enable fishery scientists and researchers to process raw image-based data at high volumes at 
		      a much quicker rate than current manual methods. This open-source dataset contains 90,000 annotations for 130 species of fish from two annual 
		      surveys done in 2018 and 2019 in strategic areas of the Gulf of Mexico using baited remote underwater video systems to assess reef fish populations.
		      We perform baseline experiments and document predictive classification results. We document this work at https://github. com/SEFSC/SEAMAPD21.
              </p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Memberships</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <a >IEEE Memberships</a>
              <br>
              <a >IEEE-Eta Kappa Nu (HKN) Memberships</a>
            </td>
          </tr>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
